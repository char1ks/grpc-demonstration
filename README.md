# Стенд: демонстрация сильных сторон gRPC

Этот стенд показывает практическую разницу между REST и gRPC на одном и том же бизнес-сценарии: клиент отправляет N измерений датчика в обработчик. В REST-варианте — много маленьких HTTP-запросов с JSON; в gRPC-варианте — один клиентский стрим в protobuf. В Grafana визуализируются метрики по байтам и времени, а также системные метрики контейнеров.

## Ценность для студента
- Понимание накладных расходов протоколов: как количество запросов и формат данных влияют на байты и задержку.
- Практика с gRPC client‑streaming и REST, сравнение «лоб в лоб» на реальном запуске.
- Навыки наблюдаемости: Prometheus, Grafana, cAdvisor; умение интерпретировать метрики.

## Состав стенда
- `client` — HTTP API для запуска двух сценариев: `/trigger/rest` и `/trigger/grpc`; собирает метрики отправленных/полученных байт и времени.
- `processor-rest` — REST обработчик (`POST /process`), считает базовую функцию и отдаёт ответ.
- `processor-grpc` — gRPC сервер с клиентским стримом `ProcessStream`, агрегирует данные и возвращает статистику.
- `prometheus` — сбор метрик приложений и cAdvisor.
- `grafana` — готовый дашборд «REST vs gRPC — байты и время».
- `cadvisor` — системные метрики контейнеров: CPU, сеть, I/O.

## Измеряемые метрики
- `app_send_bytes_total{protocol,service}` — суммарные отправленные байты на уровне приложения.
- `app_recv_bytes_total{protocol,service}` — суммарные полученные байты на уровне приложения.
- `app_request_duration_seconds{protocol,service}` — гистограмма длительности «запуска сценария» на клиенте.
- cAdvisor:
  - `container_cpu_usage_seconds_total` — использование CPU контейнеров.
  - `container_network_receive_bytes_total` / `container_network_transmit_bytes_total` — сетевые байты.

Примечание: учёт байт в приложении сделан по размеру сериализованного тела (`JSON.stringify(...)`), что демонстрационно отражает накладные расходы; сетевые байты на уровне контейнера показывают фактический трафик.

## Запуск примера
Требуется Docker и Docker Compose.

1. Сборка и запуск:
   - `docker compose up -d --build`
2. Открыть Grafana:
   - `http://localhost:3003` (логин/пароль по умолчанию `admin`/`admin`)
   - Дашборд: «REST vs gRPC — байты и время» (уже провиженится автоматически).
3. Выполнить сценарии через `client`:
   - REST: `curl "http://localhost:8082/trigger/rest?count=1000&size=32"`
   - gRPC: `curl "http://localhost:8082/trigger/grpc?count=1000&size=32"`
   - Параметры:
     - `count` — количество измерений (по умолчанию 1000).
     - `size` — размер текстовой «паддинга» в сообщении для демонстрации JSON‑накладных.
4. Обновить Grafana и сравнить:
   - Среднее время выполнения на клиенте по протоколам.
   - Отправленные/полученные байты на клиенте по протоколам.
   - Сетевые байты и CPU контейнеров (REST vs gRPC).

Ожидаемый результат: у REST выше суммарные отправленные байты и длительность при больших `count`; у gRPC — меньше байт и времени за счёт одного соединения и потоковой передачи.

## Небольшое доп. ДЗ
- Включить компрессию в REST (например, gzip в `client` и `processor-rest`) и сравнить влияние на байты/время.
- Добавить server‑streaming в gRPC и провести сравнение с REST long‑polling.
- Изменить `size` (например, 4, 32, 256 байт) и построить зависимость байт/времени от размера сообщения.

## Структура и запуск без технических тонкостей
- Все сервисы уже контейнеризованы и связаны в `docker-compose.yml`.
- Метрики приложений доступны на `/metrics` каждого сервиса и собираются Prometheus автоматически.
- Grafana провиженит источник данных и дашборд из `deploy/grafana/*`.
- cAdvisor публикует системные метрики контейнеров.

## Быстрые ссылки
- `client` API: `http://localhost:8082`
- Prometheus: `http://localhost:9090`
- Grafana: `http://localhost:3003`
- cAdvisor UI: `http://localhost:8081`
